{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Indices & Create Sample Files:\n",
    "### *All Tweets* with only *real/cfake* categories\n",
    "\n",
    "#### This notebook generates indices for use in sampling the data as well as exports datasets from the corpus based on those indices.  The indices are exported as well.  Additionally, the data exproted by this notebook is only categorized as either **_real_** or **_cfake_**, where **_cfake_** is a category for *all* tweets that are not genuine.\n",
    "\n",
    "## Load the Date Files\n",
    "\n",
    "First, we store the file names (*and their locations*) of the files containing fake tweets in the string '**fileName0**' and the string array '**fileName**'[ ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileName0 = '../../../../datasetsFULLcsv/fakeFollowersCSV/tweets.csv'\n",
    "\n",
    "fileNames = c('../../../../datasetsFULLcsv/socialSpambots1csv/tweets.csv', '../../../../datasetsFULLcsv/socialSpambots2csv/tweets.csv', '../../../../datasetsFULLcsv/socialSpambots3csv/tweets.csv', '../../../../datasetsFULLcsv/traditionalSpambots1csv/tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the CSV file names previously specified in '**fileName0**' and '**fileNames**'[ ], we can now load the file into the _data.frame_( ) named '**fakeCSV**'. Additionally, from the '**fakeCSV**' and **_subsequent_** _data.frame_( ), we will create a smaller, simpler *data.frame*( ) named '**fakeTweets**'.  This reduction in size and complexity of '**fakeTweets**' is due to the fact that it only contains the ID number of the tweet in our database, along with the text of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”"
     ]
    }
   ],
   "source": [
    "fakeCSV = read.csv(fileName0)\n",
    "fakeTweets <- data.frame(userID = fakeCSV$user_id, id = fakeCSV$id, text = fakeCSV$text)\n",
    "\n",
    "for (filename in fileNames) {\n",
    "    temp0 = read.csv(filename)\n",
    "    temp <- data.frame(userID = temp0$user_id, id = temp0$id, text = temp0$text)\n",
    "    fakeTweets <- rbind(fakeTweets, temp)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the file containing real tweets into the _data.frame_( ) named '**realCSV**'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”"
     ]
    }
   ],
   "source": [
    "realCSV = read.csv('../../../../datasetsFULLcsv/genuineAccountsCSV/tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the '**fakeCSV**' _data.frame_( ), from the '**realCSV** _data.frame_( ), we will create two smaller, simpler *data.frame*( ) named '**realTweets**'.  This reduction in size and complexity of '**realTweets**' is due to the fact that it only contains the ID number of the tweet in our database, along with the text of the tweet.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "realTweets <- data.frame(id = realCSV$id, text = realCSV$text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial size of the imports are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrow(fakeTweets)\n",
    "nrow(realTweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Data\n",
    "\n",
    "Now, we remove web URLs, twitter usernames, twitter hashtags, punctuation, and stand-alone numberic digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove web URLs\n",
    "fakeTweets <- data.frame(id = fakeTweets$id, text = gsub(\"http[[:alnum:][:punct:]]*\", \"\", fakeTweets$text))\n",
    "realTweets <- data.frame(id = realTweets$id, text = gsub(\"http[[:alnum:][:punct:]]*\", \"\", realTweets$text))\n",
    "\n",
    "# remove twitter handles (@<username>)\n",
    "fakeTweets <- data.frame(id = fakeTweets$id, text = gsub(\"#[[:alnum:][:punct:]]*\", \"\", fakeTweets$text))\n",
    "realTweets <- data.frame(id = realTweets$id, text = gsub(\"#[[:alnum:][:punct:]]*\", \"\", realTweets$text))\n",
    "\n",
    "# remove hashtags (#<hashtag name>)\n",
    "fakeTweets <- data.frame(id = fakeTweets$id, text = gsub(\"@[[:alnum:][:punct:]]*\", \"\", fakeTweets$text))\n",
    "realTweets <- data.frame(id = realTweets$id, text = gsub(\"@[[:alnum:][:punct:]]*\", \"\", realTweets$text))\n",
    "\n",
    "# remove punctuation\n",
    "fakeTweets <- data.frame(id = fakeTweets$id, text = gsub('[[:punct:] ]+', ' ', fakeTweets$text))\n",
    "realTweets <- data.frame(id = realTweets$id, text = gsub('[[:punct:] ]+', ' ', realTweets$text))\n",
    "\n",
    "# remove numbers\n",
    "fakeTweets <- data.frame(id = fakeTweets$id, text = gsub(\"[0-9]\", \"\", fakeTweets$text))\n",
    "realTweets <- data.frame(id = realTweets$id, text = gsub(\"[0-9]\", \"\", realTweets$text))\n",
    "\n",
    "# convert to lowercase\n",
    "fakeTweets <- data.frame(id = fakeTweets$id, text = tolower(fakeTweets$text))\n",
    "realTweets <- data.frame(id = realTweets$id, text = tolower(realTweets$text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tweets available after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrow(fakeTweets)\n",
    "nrow(realTweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TidyText the Data File\n",
    "\n",
    "Now we must tokenize the text of each tweet using the '*tidytext*' and '*dplyr*' libraries.  To do this, the '*tidytext*' and '*dplyr*' libraries must be imported and the data frames type used to store both types of tweets converted to the data frame type from the '**dplyr**' library before the tweets can be tokenized. \n",
    "\n",
    "> First, we import the '*tidytext*' and '*dplyr*' libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(tidytext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then we convert the data frames of '**fakeTweets**' and '**realTweets**' to the data frame type from the '*dplyr*' library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakeTweets <- data_frame(id = fakeTweets$id, text = as.character(fakeTweets$text))\n",
    "realTweets <- data_frame(id = realTweets$id, text = as.character(realTweets$text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "We now tokenize the text of the tweets, storing these tokens in new data frames (*from __dplyr__*). \n",
    "\n",
    "> For the fake tweets, we use the new data frame (*from __dplyr__*) '**fakeTweetTOKENS**':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakeTweetsTOKENS <- fakeTweets %>%\n",
    "    unnest_tokens(word, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Similarly, we tokenize the text of the real tweets, them in the new data frame (*from __dplyr__*) '**realTweetTOKENS**'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "realTweetsTOKENS <- realTweets %>%\n",
    "    unnest_tokens(word, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove '*Stop Words*'\n",
    "\n",
    "Now, we will remove any stop words from the text of the tweets.  To do this, we first import the '*stop_words*' dataset from the '*tidytext*' library.\n",
    "\n",
    "> Importing the '*stop_words*' dataset from the '*tidytext*' library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, we use the '*anti_join*( )' function from the '*dplyr*' library to remove these stop words from the fake tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakeTweetsTOKENS <- fakeTweetsTOKENS %>%\n",
    "    anti_join(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> and the real tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "realTweetsTOKENS <- realTweetsTOKENS %>%\n",
    "    anti_join(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of fake and real tweet tokens is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrow(fakeTweetsTOKENS)\n",
    "nrow(realTweetsTOKENS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NRC Model\n",
    "\n",
    "We are first going to use the **NRC Sentiment Model** containing ten emotions associated with words.  Thus, we load the sentiments database for NRC followed by storing those ten emotions in their own dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrcWORDS <- get_sentiments(\"nrc\")\n",
    "nrcEMOTIONS <- unique(nrcWORDS$sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate Words with their Emotions\n",
    "\n",
    "Now, we generate data frames that associates the words in fake tweets with their associated emotions under the **NRC Model**.\n",
    "\n",
    "> First, for fake tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial data.frame() to hold the processed data\n",
    "#   with an initial value for ID to make full_join()s possible\n",
    "fakeTweetsNRCsentiment <- data.frame(id = 0)\n",
    "\n",
    "# loop though the emotions\n",
    "for (emotion in nrcEMOTIONS){\n",
    "    fakeTweetsNRCsentiment0 <- inner_join(fakeTweetsTOKENS, filter(nrcWORDS, sentiment == emotion))\n",
    "    fakeTweetsNRCsentiment <- full_join(fakeTweetsNRCsentiment, fakeTweetsNRCsentiment0)\n",
    "    }\n",
    "\n",
    "# remove the initial value from the data.frame()\n",
    "fakeTweetsNRCsentiment <- data.frame(fakeTweetsNRCsentiment[-1,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> and then for real tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial data.frame() to hold the processed data\n",
    "#   with an initial value for ID to make full_join()s possible\n",
    "realTweetsNRCsentiment <- data.frame(id = as.factor(0))\n",
    "\n",
    "# loop though the emotions\n",
    "for (emotion in nrcEMOTIONS){\n",
    "    realTweetsNRCsentiment0 <- inner_join(realTweetsTOKENS, filter(nrcWORDS, sentiment == emotion))\n",
    "    realTweetsNRCsentiment <- full_join(realTweetsNRCsentiment, realTweetsNRCsentiment0)\n",
    "    }\n",
    "\n",
    "# remove the initial value from the data.frame()\n",
    "realTweetsNRCsentiment <- data.frame(realTweetsNRCsentiment[-1,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The number of word, sentiment groups now in our data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrow(fakeTweetsNRCsentiment)\n",
    "nrow(realTweetsNRCsentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate *Tweet-Emotion Matrices*\n",
    "\n",
    "We now need to generate what we are calling *Tweet-Emotion Matrices*.  In these matrices, the rows represent the tweets we have analyzed, while the columns represent the ten emotions in the **NRC Model**.  Thus, the elements of the matrix represent the strength of the emotion represented by the element's column for the tweet represented by the element's row.\n",
    "\n",
    "> First, we do this for the fake tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attach(fakeTweetsNRCsentiment)\n",
    "fakeNRCscoredTweets <- data.frame(table(id, sentiment), realFAKEcat = \"fake\")\n",
    "detach(fakeTweetsNRCsentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then we do it for the real tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attach(realTweetsNRCsentiment)\n",
    "realNRCscoredTweets <- data.frame(table(id, sentiment), realFAKEcat = \"real\")\n",
    "detach(realTweetsNRCsentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We now create a combined data frame containing **_ALL_** tweets, both real and fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NRCscoredTweets <- rbind(fakeNRCscoredTweets, realNRCscoredTweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using this data frame, we create \"data *sub*-frames\" to store the frequency data for each emotion covered by the **NRC Model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trust\n",
    "trustScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"trust\")$id, trust = filter(NRCscoredTweets, sentiment == \"trust\")$Freq)\n",
    "\n",
    "# fear\n",
    "fearScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"fear\")$id, fear = filter(NRCscoredTweets, sentiment == \"fear\")$Freq)\n",
    "\n",
    "# negative\n",
    "negScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"negative\")$id, negative = filter(NRCscoredTweets, sentiment == \"negative\")$Freq)\n",
    "\n",
    "# sadness\n",
    "sadnessScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"sadness\")$id, sadness = filter(NRCscoredTweets, sentiment == \"sadness\")$Freq)\n",
    "\n",
    "# anger\n",
    "angerScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"anger\")$id, anger = filter(NRCscoredTweets, sentiment == \"anger\")$Freq)\n",
    "\n",
    "# surprise\n",
    "surpriseScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"surprise\")$id, surprise = filter(NRCscoredTweets, sentiment == \"surprise\")$Freq)\n",
    "\n",
    "# positive\n",
    "posScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"positive\")$id, positive = filter(NRCscoredTweets, sentiment == \"positive\")$Freq)\n",
    "\n",
    "# disgust\n",
    "disgustScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"disgust\")$id, disgust = filter(NRCscoredTweets, sentiment == \"disgust\")$Freq)\n",
    "\n",
    "# joy\n",
    "joyScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"joy\")$id, joy = filter(NRCscoredTweets, sentiment == \"joy\")$Freq)\n",
    "\n",
    "# anticipation\n",
    "anticipationScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"anticipation\")$id, anticipation = filter(NRCscoredTweets, sentiment == \"anticipation\")$Freq, realFAKEcat = filter(NRCscoredTweets, sentiment == \"anticipation\")$realFAKEcat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These \"data *sub*-frames\" are now combined by the tweet id represented by each row to give the final **_Tweet-Emotion Matrix_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrcSCORES <- full_join(trustScores, full_join(fearScores, full_join(negScores, full_join(sadnessScores, full_join(angerScores, full_join(surpriseScores, full_join(posScores, full_join(disgustScores, full_join(joyScores, anticipationScores)))))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Look at the table\n",
    "\n",
    "> We now get a quick look at the layout of our table to ensure its what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrcSCORES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate fake and real tables\n",
    "\n",
    "We now create two versions of this **_Tweet-Emotion Matrix_**, one with only fake tweets and the other with only real tweets.\n",
    "\n",
    "> For the fake tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakeNRCscores <- filter(nrcSCORES, realFAKEcat == 'fake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then for the real tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "realNRCscores <- filter(nrcSCORES, realFAKEcat == 'real')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export tables\n",
    "\n",
    "We now export the complete processed tables for fake tweets, real tweets, and all tweets.\n",
    "\n",
    "> Starting with the fake tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.table(fakeNRCscores, \"fakeNRCscores.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> and continuing with the real tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.table(realNRCscores, \"realNRCscores.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> and finishing with all tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.table(nrcSCORES, \"NRCscores.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampling Indicies Generation\n",
    "\n",
    "We now need to generate sets containing random indicies of elements in our data so that we can sample our data for training and testing purposes.  We will create 4 paired sets of indicies, with two pairs of sets being for training and the remaining 2 sets being for testing.  These pairs of indicies are\n",
    "* __PAIR 1 :__ Has a total size of 50,000 between the pair of sets.  The first set in the pair is for 25,000 real tweets, while the second set is for 25,000 fake tweets.  This pair of sets is for training\n",
    "* __PAIR 2 :__ Has a total size of 10,000 between the pair of sets.  The first set in the pair is for 5,000 real tweets, while the second set is for 5,000 fake tweets. This pair of sets if for training\n",
    "* __PAIR 3 :__ Has a total size of 15,000 and randomly contains any tweet in the database.  This set is for testing\n",
    "* __PAIR 4 :__ Has a total size of 5,000 and randomly contains any tweet in the database.  This set is for testing\n",
    "\n",
    "### *PAIR 1*\n",
    "\n",
    "> Seeding the RNG and generating the first pair of index sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(runif(1,1,10000))\n",
    "fakeTrainInd1 <- sample(1:nrow(fakeNRCscores), 25000)\n",
    "\n",
    "set.seed(runif(1,1,10000))\n",
    "realTrainInd1 <- sample(1:nrow(realNRCscores), 25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the first pair of index sets to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.table(fakeTrainInd1, \"fakeTrainInd1.txt\")\n",
    "\n",
    "write.table(realTrainInd1, \"realTrainInd1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAIR 2\n",
    "\n",
    "> Seeding the RNG and generating the second pair of index sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(runif(1,1,10000))\n",
    "fakeTrainInd2 <- sample(1:nrow(fakeNRCscores), 5000)\n",
    "\n",
    "set.seed(runif(1,1,10000))\n",
    "realTrainInd2 <- sample(1:nrow(realNRCscores), 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Writing the second pair of index sets to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.table(fakeTrainInd2, \"fakeTrainInd2.txt\")\n",
    "\n",
    "write.table(realTrainInd2, \"realTrainInd2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAIR 3\n",
    "\n",
    "> Seeding the RNG and generating the first index set for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(runif(1,1,10000))\n",
    "testInd1 <- sample(1:nrow(nrcSCORES), 15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> write the first index set for testing to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.table(testInd1, \"testInd1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAIR 4\n",
    "\n",
    "> Seeding the RNG and generating the second index set for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(runif(1,1,10000))\n",
    "testInd2 <- sample(1:nrow(nrcSCORES), 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> write the second index set for testing to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.table(testInd2, \"testInd2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Sampled Data\n",
    "\n",
    "We can now use that previously created sampling indices to selected subsets of our overall dataset which we can then write to files.\n",
    "\n",
    "> Selecting data subsets using the first training sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakeTrainData1 <- fakeNRCscores[fakeTrainInd1, ]\n",
    "\n",
    "realTrainData1 <- realNRCscores[realTrainInd1, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Storing these data subsets containing the first training sets, buy writing them to files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.table(fakeTrainData1, \"fakeTrainData1.csv\", sep=\",\")\n",
    "\n",
    "write.table(realTrainData1, \"realTrainData1.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Selecting data subsets using the second training sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakeTrainData2 <- fakeNRCscores[fakeTrainInd2, ]\n",
    "\n",
    "realTrainData2 <- realNRCscores[realTrainInd2, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Storing these data subsets containing the second training sets, buy writing them to files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.table(fakeTrainData2, \"fakeTrainData2.csv\", sep=\",\")\n",
    "\n",
    "write.table(realTrainData2, \"realTrainData2.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Selecting data subsets using the first testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testData1 <- nrcSCORES[testInd1, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Storing this data subset containing the first testing set, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.table(testData1, \"testData.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Selecting data subsets using the second testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testData2 <- nrcSCORES[testInd2, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Storing this data subset containing the first testing set, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.table(testData2, \"testData2.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
